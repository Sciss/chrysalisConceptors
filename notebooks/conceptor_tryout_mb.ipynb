{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run conceptors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# making some waveform patterns that we might use\n",
    "pSaw = lambda n: (round(n % waveLengthSamples) / waveLengthSamples * 2) - 1.0\n",
    "pPulse = lambda n: (((n % waveLengthSamples) < (waveLengthSamples * 0.5)) * 2) - 1.0\n",
    "pSine2 = lambda n: (np.sin(n) * np.sin((n+np.pi/4)/6))\n",
    "pSine3 = lambda n: (np.sin(n) * np.sin((n/4)/6)/6)\n",
    "pJ1 = lambda n: 1 * np.sin(2 * np.pi * n / 3.1504531)\n",
    "pJ1b = lambda n: 1 * np.sin(n/2) ** 1\n",
    "\n",
    "period2 = 2\n",
    "rawp = np.random.randn(period2)\n",
    "# rawp = np.array([1.1929,2.6856]);\n",
    "maxVal = np.max(rawp)\n",
    "minVal = np.min(rawp)\n",
    "rp = 0.5 * (2 * (rawp - minVal) / (maxVal - minVal) - 1);\n",
    "pJ2 = lambda n: rp[np.mod(n, period2 )]\n",
    "pTri = lambda n,p: (((n % p) >= (p/2)) * ((p/2) - (n % (p/2))) + ((n % p) < (p/2)) * (n % (p/2))) * (2/p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(20,4)\n",
    "# plot([pJ1b(x) for x in arange(15)])\n",
    "# plot([pJ1(x) for x in arange(15)])\n",
    "# plot([pJ2(x) for x in arange(15)])\n",
    "plot([pSine2(x) for x in arange(15)])\n",
    "\n",
    "# plot([pTri(x,10) for x in arange(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'N':32,\n",
    "          'Nout':2,\n",
    "          'NetSR':0.6, # was 1.6; scaling for internal weights: according to ESN tutorial, this one should actually be < 1\n",
    "          'NetinpScaling':1.6, # scaling for input weights\n",
    "          'BiasScaling':0.3, # scaling for bias\n",
    "          'TychonovAlpha':0.0001,\n",
    "          'TychonovAlphaReadout':0.0001,\n",
    "          'washoutLength':100, 'learnLength':500, \n",
    "          'learnLengthWout':500, \n",
    "          'recallTestLength':100,\n",
    "          'alphas':np.array([12.0,24.0,36.0]),\n",
    "          'patts':np.array([pJ1b, pJ2,pSine2])\n",
    "}\n",
    "\n",
    "net1 = makeNetwork(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = makeNetwork(params)\n",
    "net3 = makeNetwork(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphValues = [1,0,0,1,0,0,1,0,0]\n",
    "\n",
    "def onMorph(ind,f):\n",
    "    global morphValues\n",
    "#     print(\"onMorph\", f)\n",
    "    morphValues[ind] = f\n",
    "    return morphValues\n",
    "\n",
    "def onExit():\n",
    "    print( \"exiting\" )\n",
    "    global keepRunning\n",
    "    keepRunning = False\n",
    "    return True\n",
    "\n",
    "oscserver = makeOSCServer(onMorph,onExit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "keepRunning = True\n",
    "\n",
    "#run a network\n",
    "x1 = 0.5 * np.random.randn(net1['p']['N'],1)\n",
    "x2 = 0.5 * np.random.randn(net2['p']['N'],1)\n",
    "x3 = 0.5 * np.random.randn(net3['p']['N'],1)\n",
    "\n",
    "while keepRunning:\n",
    "#     print( morphValues )\n",
    "    # net 1\n",
    "    output1, x1 = conceptor_mix_step( net1, x1, morphValues[0:3] )\n",
    "    oscserver.send_value( \"/output/1\", output1[0][0] )\n",
    "    oscserver.send_array( \"/x/1\", x1.flatten() )\n",
    "    \n",
    "    # net 2\n",
    "    output2, x2 = conceptor_mix_step( net2, x2, morphValues[3:6] )\n",
    "    oscserver.send_value( \"/output/2\", output2[0][0] )\n",
    "    oscserver.send_array( \"/x/2\", x2.flatten() )\n",
    "\n",
    "    # net 3\n",
    "    output3, x3 = conceptor_mix_step( net3, x3, morphValues[6:9] )\n",
    "    oscserver.send_value( \"/output/3\", output3[0][0] )\n",
    "    oscserver.send_array( \"/x/3\", x3.flatten() )\n",
    "    \n",
    "    time.sleep(0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conceptor_mix_step( net, x, morphvalues, oversample=1 ):\n",
    "    ind = 0\n",
    "    C = np.zeros( (net['p']['N'] , net['p']['N'] ) )\n",
    "    for i_morph in morphvalues:\n",
    "        C = C + (net['Cs'][0,ind].dot( i_morph ))\n",
    "        ind = ind + 1\n",
    "    Wsr = net['W'].dot(1.2)\n",
    "    for i_oversample in range( oversample ):\n",
    "        x = np.tanh(Wsr.dot(x) + net['Wbias'])\n",
    "        x = C.dot(x)\n",
    "    output = net1['Wout'].dot(x)\n",
    "    return output, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( net1['p']['N'] )\n",
    "print( morphValues[0:3], morphValues[3:6], morphValues[6:9])\n",
    "morphFactors = morphValues[0:3]\n",
    "a, b = conceptor_mix_step( net1, x1, morphFactors )\n",
    "print( a )\n",
    "print( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test OSC\n",
    "try:\n",
    "    target = Address(57400)\n",
    "except AddressError as err:\n",
    "    print (str(err))\n",
    "    sys.exit()\n",
    "\n",
    "# send message \"/foo/message1\" with int, float and string arguments\n",
    "send(target, \"/morph\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [ 0, 2, 4]\n",
    "oscserver.send_array(\"/morph\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = {'N':5,\n",
    "          'Nout':2,\n",
    "          'NetSR':0.6, # was 1.6; scaling for internal weights: according to ESN tutorial, this one should actually be < 1\n",
    "          'NetinpScaling':1.6, # scaling for input weights\n",
    "          'BiasScaling':0.3, # scaling for bias\n",
    "          'TychonovAlpha':0.0001,\n",
    "          'TychonovAlphaReadout':0.0001,\n",
    "          'washoutLength':100, 'learnLength':500, \n",
    "          'learnLengthWout':500, \n",
    "          'recallTestLength':100,\n",
    "          'alphas':np.array([12.0,24.0,36.0]),\n",
    "          'patts':np.array([pJ1b, pJ2,pSine2])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetConnectivity = 1 # just for small networks\n",
    "if p['N'] > 20:\n",
    "    NetConnectivity = 10.0/p['N'];\n",
    "WstarRaw = generateInternalWeights(p['N'], NetConnectivity)\n",
    "WinRaw = np.random.randn(p['N'], 1) # input is 1-dimensional\n",
    "WbiasRaw = np.random.randn(p['N'], 1) # bias is also 1-dimensional\n",
    "\n",
    "#Scale raw weights\n",
    "Wstar = p['NetSR'] * WstarRaw;\n",
    "Win = p['NetinpScaling'] * WinRaw;\n",
    "Wbias = p['BiasScaling'] * WbiasRaw;\n",
    "I = np.eye(p['N']) # identity matrix\n",
    "\n",
    "xCollector = np.zeros((p['N'],p['Nout'], p['learnLengthWout'])) # variable to collect states of x\n",
    "pCollector = np.zeros((p['Nout'], p['learnLengthWout'])) # variable to collect states of p (output?)\n",
    "x = np.zeros((p['N'],p['Nout'])) # initial state\n",
    "\n",
    "\n",
    "# first training: washout is to wash out the input state 'noise'; learnLength is then the actual amount of learning samples\n",
    "for n in np.arange(p['washoutLength'] + p['learnLength']):\n",
    "    u = np.random.randn() * 1.5  # random input\n",
    "    print( Win , u )\n",
    "    print( Win.dot(u) )\n",
    "    print( Wstar.dot(x) )\n",
    "    x = np.tanh( Wstar.dot(x) + Win.dot(u) + Wbias ) # calculate next internal activation\n",
    "    if n >= p['washoutLength']:\n",
    "        xCollector[:,:, n - p['washoutLength']] = x[:,:]\n",
    "        pCollector[:, n - p['washoutLength']] = u\n",
    "        \n",
    "print( pCollector.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "gesture1 = np.loadtxt(cwd + '/workshop/gesturedata_0.txt')\n",
    "\n",
    "print( gesture1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nescivi/git/projects/sussex/chrysalisConceptors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/nescivi/git/projects/sussex/chrysalisConceptors/supercollider/workshop/gesturedata_0.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "cwd + '/workshop/gesturedata_0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def makeNetwork(p):    \n",
    "    NetConnectivity = 1 # just for small networks\n",
    "    if p['N'] > 20:\n",
    "        NetConnectivity = 10.0/p['N'];\n",
    "    WstarRaw = generateInternalWeights(p['N'], NetConnectivity)\n",
    "    WinRaw = np.random.randn(p['N'], p['Nout'])\n",
    "    WbiasRaw = np.random.randn(p['N'], p['Nout'])\n",
    "\n",
    "    #Scale raw weights\n",
    "    Wstar = p['NetSR'] * WstarRaw;\n",
    "    Win = p['NetinpScaling'] * WinRaw;\n",
    "    Wbias = p['BiasScaling'] * WbiasRaw;\n",
    "    I = np.eye(p['N']) # identity matrix\n",
    "    \n",
    "    xCollector = np.zeros((p['N'], p['learnLengthWout'])) # variable to collect states of x\n",
    "    pCollector = np.zeros((p['Nout'], p['learnLengthWout'])) # variable to collect states of p (output?)\n",
    "    x = np.zeros((p['N'],p['Nout'])) # initial state\n",
    "    \n",
    "    # first training: washout is to wash out the input state 'noise'; learnLength is then the actual amount of learning samples\n",
    "    for n in np.arange(p['washoutLength'] + p['learnLength']):\n",
    "        u = np.random.randn(p['Nout']) * 1.5  # random input\n",
    "        x = np.tanh( Wstar.dot(x) + Win.dot(u) + Wbias ) # calculate next internal activation\n",
    "        if n >= p['washoutLength']:\n",
    "            xCollector[:, n - p['washoutLength']] = x[:,0]\n",
    "            pCollector[:, n - p['washoutLength']] = u\n",
    "\n",
    "#     print(\"Mean/Max/Min Activations, random network driven by noise\")\n",
    "#     plot(np.mean(xCollector.T, axis=1))\n",
    "#     plot(np.max(xCollector.T, axis=1))\n",
    "#     plot(np.min(xCollector.T, axis=1))\n",
    "\n",
    "    # Wout \n",
    "    Wout = np.linalg.inv( xCollector.dot(xCollector.conj().T) + ( p['TychonovAlphaReadout'] * np.eye(p['N']) ) ).dot(xCollector).dot(pCollector.conj().transpose()).conj().T\n",
    "    print(\"Initial training\")\n",
    "    print(\"NRMSE: \", nrmse(Wout.dot(xCollector), pCollector))\n",
    "    print(\"absWeight: \", np.mean(abs(Wout)))\n",
    "\n",
    "    allTrainArgs = np.zeros((p['N'], p['patts'].size * p['learnLength']))\n",
    "    allTrainOldArgs = np.zeros((p['N'], p['patts'].size * p['learnLength']))\n",
    "    allTrainTargs = np.zeros((p['N'], p['patts'].size * p['learnLength']))\n",
    "    allTrainOuts = np.zeros((p['Nout'], p['patts'].size * p['learnLength']))\n",
    "    xCollectors =  np.zeros((p['Nout'], p['patts'].size), dtype=np.object)\n",
    "    SRCollectors =  np.zeros((p['Nout'], p['patts'].size), dtype=np.object)\n",
    "    URCollectors =  np.zeros((p['Nout'], p['patts'].size), dtype=np.object)\n",
    "    patternRs =  np.zeros((p['Nout'], p['patts'].size), dtype=np.object)\n",
    "    #train_xPL =  np.zeros((1, p['patts'].size), dtype=np.object)\n",
    "    #train_pPL =  np.zeros((1, p['patts'].size), dtype=np.object)\n",
    "    startXs =  np.zeros((p['N'], p['patts'].size), dtype=np.object)\n",
    "\n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        print('Loading pattern ', i_pattern)\n",
    "        patt = p['patts'][i_pattern]\n",
    "        xCollector = np.zeros((p['N'], p['learnLength']))\n",
    "        xOldCollector = np.zeros((p['N'], p['learnLength']))\n",
    "        pCollector = np.zeros((p['Nout'], p['learnLength']))\n",
    "        x = np.zeros((p['N'],1))\n",
    "        for n in range(p['washoutLength'] + p['learnLength']):\n",
    "            u = patt(n+1)\n",
    "            xOld = x\n",
    "            x = np.tanh( Wstar.dot(x) + Win.dot(u) + Wbias )\n",
    "            if n >= p['washoutLength']:\n",
    "                xCollector[:, n - p['washoutLength']] = x[:,0]\n",
    "                xOldCollector[:, n - p['washoutLength']] = xOld[:,0]\n",
    "                pCollector[:, n - p['washoutLength']] = u\n",
    "\n",
    "        xCollectors[0,i_pattern] = xCollector\n",
    "        R = xCollector.dot(xCollector.T) / p['learnLength']\n",
    "        [Ux,sx,Vx] = np.linalg.svd(R)\n",
    "        SRCollectors[0,i_pattern] = np.diag(sx)\n",
    "        URCollectors[0,i_pattern] = Ux\n",
    "        patternRs[0,i_pattern] = R\n",
    "\n",
    "        startXs[:,i_pattern] = x[:,0]\n",
    "\n",
    "        #needed?\n",
    "        #train_xPL[0,i_pattern] = xCollector[:,:signalPlotLength]\n",
    "        #train_pPL[0,i_pattern] = pCollector[0,:signalPlotLength]\n",
    "        ###\n",
    "\n",
    "        allTrainArgs[:, i_pattern * p['learnLength']:(i_pattern+1) * p['learnLength']] = xCollector\n",
    "        allTrainOldArgs[:, i_pattern * p['learnLength']:(i_pattern+1) * p['learnLength']] = xOldCollector\n",
    "        allTrainOuts[:, i_pattern * p['learnLength']:(i_pattern+1) * p['learnLength']] = pCollector\n",
    "        allTrainTargs[:, i_pattern * p['learnLength']:(i_pattern+1) * p['learnLength']] = Win.dot(pCollector)\n",
    "\n",
    "    Wtargets = np.arctanh(allTrainArgs) - np.tile( Wbias, (1, p['patts'].size * p['learnLength']))\n",
    "\n",
    "    W = np.linalg.inv(allTrainOldArgs.dot(allTrainOldArgs.conj().T) +\n",
    "                      (p['TychonovAlpha'] * np.eye(p['N']))).dot(allTrainOldArgs).dot(Wtargets.conj().T).conj().T\n",
    "    print(\"W NMRSE: \", np.mean(nrmse(W.dot(allTrainOldArgs), Wtargets)))\n",
    "    print(\"absSize: \", np.mean(np.mean(abs(W), axis=0)))\n",
    "\n",
    "    # figure(1)\n",
    "    # plot(np.mean(W.dot(allTrainOldArgs).T, axis=1))\n",
    "\n",
    "    print('Computing conceptors')\n",
    "\n",
    "    Cs = np.zeros((4, p['patts'].size), dtype=np.object)\n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        R = patternRs[0,i_pattern]\n",
    "        [U,s,V] = np.linalg.svd(R)\n",
    "        S = np.diag(s)\n",
    "        Snew = (S * np.linalg.inv(S + pow(p['alphas'][i_pattern], -2) * np.eye(p['N'])))\n",
    "\n",
    "        C =  U.dot(Snew).dot(U.T);\n",
    "        Cs[0,i_pattern] = C\n",
    "        Cs[1,i_pattern] = U\n",
    "        Cs[2,i_pattern] = np.diag(Snew)\n",
    "        Cs[3,i_pattern] = np.diag(S)\n",
    "\n",
    "    x_CTestPL = np.zeros((3, p['recallTestLength'], p['patts'].size))\n",
    "    p_CTestPL = np.zeros((1, p['recallTestLength'], p['patts'].size))\n",
    "    for i_pattern in range(p['patts'].size):\n",
    "        C = Cs[0,i_pattern]\n",
    "        x = 0.5 * np.random.randn(p['N'],1)\n",
    "        for n in range(p['recallTestLength'] + p['washoutLength']):\n",
    "            x = np.tanh(W.dot(x) + Wbias)\n",
    "            x = C.dot(x)\n",
    "            if (n > p['washoutLength']):\n",
    "                x_CTestPL[:,n-p['washoutLength'],i_pattern] = x[0:3].T\n",
    "                p_CTestPL[:,n-p['washoutLength'],i_pattern] = Wout.dot(x)\n",
    "    # for i_pattern in range(p['patts'].size):\n",
    "    #     figure(2 + i_pattern)\n",
    "    #     plot(p_CTestPL[:,:,i_pattern].T)\n",
    "    #     plot([p['patts'][i_pattern](x) for x in arange(p['recallTestLength'])])\n",
    "\n",
    "    return locals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
